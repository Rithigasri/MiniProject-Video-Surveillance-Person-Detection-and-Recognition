{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3387458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import dlib\n",
    "import numpy as np\n",
    "video_file='Output/v5.mov'\n",
    "output_dir=\"Output/Result\"\n",
    "os.makedirs(output_dir,exist_ok=True)\n",
    "cap=cv2.VideoCapture(video_file)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error:Could not open video file\")\n",
    "    exit()\n",
    "\n",
    "face_rec_model = dlib.face_recognition_model_v1(\"dlib_face_recognition_resnet_model_v1/dlib_face_recognition_resnet_model_v1.dat\")\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "input_image = cv2.imread(\"Output/inputperson1.png\")\n",
    "input_image_rgb = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "input_faces = face_detector(input_image_rgb)\n",
    "\n",
    "if len(input_faces) == 0:\n",
    "    print(\"No faces found in the input image.\")\n",
    "else:\n",
    "    landmarks = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")(input_image_rgb, input_faces[0])\n",
    "    input_face_descriptor = face_rec_model.compute_face_descriptor(input_image_rgb, landmarks)\n",
    "\n",
    "frame_rate=int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_count=0\n",
    "frame_number=1\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_count+=1\n",
    "    if frame_count%frame_rate==0:\n",
    "        frame_name = f\"{output_dir}/frame_{frame_number:04d}.jpg\"\n",
    "        frame_number += 1\n",
    "        frame_faces=face_detector(frame)\n",
    "        for face in frame_faces:\n",
    "            landmarks=dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")(frame,face)\n",
    "            frame_descriptor=face_rec_model.compute_face_descriptor(frame,landmarks)\n",
    "            distance = np.linalg.norm(np.array(input_face_descriptor) - np.array(frame_descriptor))\n",
    "            if distance < 0.4:\n",
    "#                 output_path = os.path.join(output_dir, frame_filename)\n",
    "                x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                cv2.imwrite(frame_name, frame)\n",
    "                print(f\"Match found in {frame_name},{distance}\")\n",
    "            else:\n",
    "                 print(\"Match not found\")\n",
    "                 \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7da8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clothing Color Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9363e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found in Output/Clothing/frame_0001.jpg\n",
      "Match found in Output/Clothing/frame_0003.jpg\n",
      "Match found in Output/Clothing/frame_0004.jpg\n",
      "Match found in Output/Clothing/frame_0005.jpg\n",
      "Match found in Output/Clothing/frame_0135.jpg\n",
      "Match found in Output/Clothing/frame_0163.jpg\n",
      "Match found in Output/Clothing/frame_0164.jpg\n",
      "Match found in Output/Clothing/frame_0174.jpg\n",
      "Match found in Output/Clothing/frame_0280.jpg\n",
      "Match found in Output/Clothing/frame_0310.jpg\n",
      "Match found in Output/Clothing/frame_0332.jpg\n",
      "Match found in Output/Clothing/frame_0366.jpg\n",
      "Match found in Output/Clothing/frame_0367.jpg\n",
      "Match found in Output/Clothing/frame_0573.jpg\n",
      "Match found in Output/Clothing/frame_0628.jpg\n",
      "Match found in Output/Clothing/frame_0629.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load the input image\n",
    "input_image = cv2.imread('Output/inputperson1.png')\n",
    "output_dir=\"Output/Clothing\"\n",
    "# Create a histogram for the input image\n",
    "input_hist = cv2.calcHist([input_image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "input_hist = cv2.normalize(input_hist, input_hist).flatten()\n",
    "\n",
    "# Open the video file\n",
    "video_capture = cv2.VideoCapture('Output/v5.mov')\n",
    "frame_rate=int(video_capture.get(cv2.CAP_PROP_FPS))\n",
    "frame_count=0\n",
    "frame_number=1\n",
    "net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "\n",
    "# Load COCO class names\n",
    "with open('coco.names', 'r') as f:\n",
    "    classes = f.read().strip().split('\\n')\n",
    "\n",
    "# Define the output layer names\n",
    "output_layers = net.getUnconnectedOutLayersNames()\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count+=1\n",
    "    if frame_count%frame_rate==0:\n",
    "        frame_name = f\"{output_dir}/frame_{frame_number:04d}.jpg\"\n",
    "        frame_number += 1\n",
    "        \n",
    "        blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        outs = net.forward(output_layers)\n",
    "\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5 and class_id == 0:  # Class ID 0 corresponds to 'person'\n",
    "                    center_x = int(detection[0] * frame.shape[1])\n",
    "                    center_y = int(detection[1] * frame.shape[0])\n",
    "                    w = int(detection[2] * frame.shape[1])\n",
    "                    h = int(detection[3] * frame.shape[0])\n",
    "\n",
    "                    # Calculate the coordinates of the bounding box\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Non-maximum suppression to remove duplicate detections\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "        for i in range(len(boxes)):\n",
    "            if i in indices:\n",
    "                x, y, w, h = boxes[i]\n",
    "\n",
    "                # Extract the region of interest (ROI) around the detected person\n",
    "                person_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "                # Calculate a color histogram for the detected person\n",
    "                person_hist = cv2.calcHist([person_roi], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "                person_hist = cv2.normalize(person_hist, person_hist).flatten()\n",
    "                \n",
    "                bhattacharyya_distance = cv2.compareHist(input_hist, person_hist, cv2.HISTCMP_BHATTACHARYYA)\n",
    "\n",
    "                threshold=0.4\n",
    "                # If the distance is below the threshold, save the frame\n",
    "                if bhattacharyya_distance < threshold:\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 8)\n",
    "                    cv2.imwrite(frame_name, frame)\n",
    "                    print(f\"Match found in {frame_name}\")\n",
    "#                 else:\n",
    "#                      print(\"Match not found\")\n",
    "\n",
    "\n",
    "\n",
    "# Release the video capture and writer\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc340be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c3e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "# Set the paths for the input files and the output directory\n",
    "video_file = 'Output/v5.mov'\n",
    "input_face_image = 'Output/inputperson1.png'\n",
    "output_dir = \"Output/Result\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize face recognition models\n",
    "face_rec_model = dlib.face_recognition_model_v1(\"dlib_face_recognition_resnet_model_v1/dlib_face_recognition_resnet_model_v1.dat\")\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "input_image = cv2.imread(input_face_image)\n",
    "input_image_rgb = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
    "input_faces = face_detector(input_image_rgb)\n",
    "\n",
    "if len(input_faces) == 0:\n",
    "    print(\"No faces found in the input image.\")\n",
    "else:\n",
    "    landmarks = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")(input_image_rgb, input_faces[0])\n",
    "    input_face_descriptor = face_rec_model.compute_face_descriptor(input_image_rgb, landmarks)\n",
    "\n",
    "# Clothing matching using YOLO object detection\n",
    "net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "with open('coco.names', 'r') as f:\n",
    "    classes = f.read().strip().split('\\n')\n",
    "output_layers = net.getUnconnectedOutLayersNames()\n",
    "\n",
    "# Open video file for processing\n",
    "video_capture = cv2.VideoCapture(video_file)\n",
    "frame_rate = int(video_capture.get(cv2.CAP_PROP_FPS))\n",
    "frame_count = 0\n",
    "frame_number = 1\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % frame_rate == 0:\n",
    "        frame_name = f\"{output_dir}/frame_{frame_number:04d}.jpg\"\n",
    "        frame_number += 1\n",
    "\n",
    "        # Initialize variables to keep track of matches for the entire frame\n",
    "        match_found = False\n",
    "\n",
    "        # Face recognition part\n",
    "        frame_faces = face_detector(frame)\n",
    "        for face in frame_faces:\n",
    "            landmarks = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")(frame, face)\n",
    "            frame_descriptor = face_rec_model.compute_face_descriptor(frame, landmarks)\n",
    "            distance = np.linalg.norm(np.array(input_face_descriptor) - np.array(frame_descriptor))\n",
    "\n",
    "        # YOLO object detection for clothing matching\n",
    "        blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        outs = net.forward(output_layers)\n",
    "\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5 and class_id == 0:  # Class ID 0 corresponds to 'person'\n",
    "                    center_x = int(detection[0] * frame.shape[1])\n",
    "                    center_y = int(detection[1] * frame.shape[0])\n",
    "                    w = int(detection[2] * frame.shape[1])\n",
    "                    h = int(detection[3] * frame.shape[0])\n",
    "\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    person_roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "                    person_hist = cv2.calcHist([person_roi], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "                    person_hist = cv2.normalize(person_hist, person_hist).flatten()\n",
    "\n",
    "                    bhattacharyya_distance = cv2.compareHist(input_hist, person_hist, cv2.HISTCMP_BHATTACHARYYA)\n",
    "                    threshold = 0.4\n",
    "\n",
    "                    if bhattacharyya_distance < threshold and distance < threshold:\n",
    "                        match_found = True\n",
    "\n",
    "        # Check if any match is found for the entire frame\n",
    "        if match_found:\n",
    "            # Draw bounding box and save the frame\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 8)\n",
    "            cv2.imwrite(frame_name, frame)\n",
    "            print(f\"Match found in {frame_name}, Face Distance: {distance}, Clothing Distance: {bhattacharyya_distance}\")\n",
    "        else:\n",
    "            print(\"Match not found\")\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2a0da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
